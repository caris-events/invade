# Context-Aware Matching Pipeline

This document outlines the layered strategy we use to decide whether a vocab highlight should be rendered when the surrounding context is ambiguous. The goals are:

- keep the default rule-based matcher fast and deterministic;
- augment edge cases with learned signals derived from real corpora;
- support future semantic models without reworking the build/runtime contract.

## Overview

```
┌──────────┐      ┌────────┐      ┌─────────────┐      ┌──────────────┐
│  Rules   │ ─70%→│ Pass   │─✓→   │ Highlight   │      │ Tooltip etc. │
│ (YAML)   │      │/?Score │      └─────────────┘      └──────────────┘
└────┬─────┘      └────────┘
     │ borderline
     ▼
┌─────────────┐   borderline   ┌────────────────┐
│ TF-IDF /    │───────────────▶│  Semantic      │
│ POS Model   │                │  Embeddings    │
│ (LogReg /   │◀───────────────│  (ONNX+WASM)   │
│ Naive Bayes)│     cache      └────────────────┘
└─────────────┘
```

1. **Rule filter** – existing `matchOptions` (skip phrases + weighted features) stay unchanged. They should filter ≥70 % of cases.
2. **Classifier layer** – if the rule stage cannot reach a confident score (the result lands in a configurable `uncertainRange`), we evaluate a lightweight classifier trained offline from labelled corpora.
3. **Semantic layer** – if the classifier still leaves the result undecided, we query a quantized sentence-transformer (ONNX) hosted in the extension. Only a curated set of “high-risk” vocabularies triggers the semantic model; results are cached.

## Data Model (YAML → JSON)

`database/vocabs/*.yml` now accept extra optional fields under `matchOptions.context`:

```yaml
matchOptions:
  context:
    baseScore: -1
    threshold: 0
    uncertainRange:
      min: -0.5
      max: 0.5
    classifier:
      strategy: logreg
      version: 1
      window: 3
      bias: -0.2
      threshold: 0.0
      features:
        token:prev:設定: 0.8
        token:window:餐廳: -0.9
        pos:next:NN: 0.4
      metadata:
        corpus: tools/menu_classifier/data/menu_contexts.jsonl
        updatedAt: "2025-03-01T10:00:00Z"
    semantic:
      model: mini-st-onboard
      enabled: true
      window: 6
      threshold: 0.58
      prototypes:
        - label: technology
          vector: [0.02, -0.11, …]
        - label: food
          vector: [-0.12, 0.04, …]
      highRiskOnly: true
```

All new keys are optional. Existing YAML files remain valid.

The build step copies these fields into `browser-extension/chrome/data/vocabs.json` so the content script can make decisions without additional network calls.

## Build-Time Flow

1. `tools/menu_classifier/build_model.py` ingests labelled corpora, extracts 2–3 token windows around target words, computes TF‑IDF vectors (with jieba POS tags), and trains either logistic regression or naive Bayes. The script writes per-word classifier payloads to `tools/menu_classifier/output/*.json`.
2. `cmd/build` reads those payloads and merges them into the in-memory vocab structures before serialising the extension payload.
3. Optional semantic prototypes are generated by a separate script (`tools/menu_classifier/export_semantics.py`, placeholder for now) which quantises sentence-transformer vectors and stores them in the same output directory.

## Runtime Flow

The content script (`content.js`) receives the merged `matchOptions.context` object. Decision logic:

1. Run the existing rule scorer (`evaluateContextFeatures`). If score ≥ threshold → highlight, if score < min threshold → skip.
2. If the score falls inside `uncertainRange`, run `evaluateContextClassifier`:
   - Build sparse feature vector from the segmented context (tokens before/after, POS tags, fallback to regex tokens if the segmenter is unavailable).
   - Compute `logit = bias + Σ(weight × featureValue)`; convert to probability; compare with `classifier.threshold`.
   - Annotate debug info when `settings.debugWeights` is enabled.
3. If still uncertain and `semantic.enabled` is true, call `semanticModel.evaluate(...)`:
   - Lazily load the ONNX model (`onnxruntime-web` + WASM backend).
   - Encode the local sentence window into a vector.
   - Compare to stored prototypes via cosine similarity. If the winning prototype exceeds `semantic.threshold`, adopt that label; otherwise leave undecided.
4. Cache both classifier and semantic results per DOM node to avoid recalculation on future passes.

## Caching & Performance

- Classifier weights are tiny (<5 KB per vocab) and computed synchronously.
- Semantic inference is gated by `semantic.highRiskOnly` and the rule/classifier verdict. We also memoise `(vocab.word, normalizedContext)` pairs.
- The ONNX model should be ≤5 MB; use dynamic import and `createWasmBackend()` to keep initial load fast.

## Testing Hooks

`tools/menu_classifier/smoke_test.py` (to be added) will run a micro-evaluation over holdout sentences. The build CI can call it to ensure weights exist before packaging.

## Open Items

- Integrate real corpora and ensure jieba POS tagging covers Traditional Chinese inputs.
- Quantise and ship the chosen sentence-transformer model (pending model selection).
- Wire semantic cache invalidation when the DOM mutates significantly.
